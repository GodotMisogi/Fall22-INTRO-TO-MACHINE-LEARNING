{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Optimization and Linear Regression\n",
    "\n",
    "Adapted from Applied Machine Learning Lecture Notes of Volodymyr Kuleshov, Cornel Tech\n",
    "\n",
    "**Instructor Tan Bui**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08ka9qHWnn6J",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Optimization and Calculus Background\n",
    "\n",
    "In the previous lecture, we learned what is a supervised machine learning problem.\n",
    "\n",
    "Before we turn our attention to Linear Regression, we will first dive deeper into the question of optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Components of A Supervised Machine Learning Problem\n",
    "\n",
    "At a high level, a supervised machine learning problem has the following structure:\n",
    "\n",
    "$$ \\text{Dataset} + \\underbrace{\\text{Learning Algorithm}}\\_\\text{Model Class + Objective + Optimizer } \\to \\text{Predictive Model} $$\n",
    "\n",
    "The predictive model is chosen to model the relationship between inputs and targets. For instance, it can predict future targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizer: Notation\n",
    "\n",
    "At a high-level an optimizer takes\n",
    "\n",
    "- an objective $J$ (also called a loss function) and\n",
    "- a model class $\\mathcal{M}$ and finds a model $f \\in \\mathcal{M}$ with the smallest value of the objective $J$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{f \\in \\mathcal{M}} J(f)\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, this is the function that bests \"fits\" the data on the training dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,...,n\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use the a quadratic function as our running example for an objective $J$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def quadratic_function(theta):\n",
    "    \"\"\"The cost function, J(theta).\"\"\"\n",
    "    return 0.5*(2*theta-1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can visualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First construct a grid of theta1 parameter pairs and their corresponding\n",
    "# cost function values.\n",
    "thetas = np.linspace(-0.2,1,10)\n",
    "f_vals = quadratic_function(thetas[:,np.newaxis])\n",
    "\n",
    "plt.plot(thetas, f_vals)\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Objective value')\n",
    "plt.title('Simple quadratic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: Derivatives\n",
    "\n",
    "Recall that the derivative $$\\frac{d f(\\theta^*)}{d \\theta}$$ of a univariate function $f : \\mathbb{R} \\to \\mathbb{R}$ is the instantaneous rate of change of the function $f(\\theta)$ with respect to its parameter $\\theta$ at the point $\\theta^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def quadratic_derivative(theta):\n",
    "    return (2*theta-1)*2\n",
    "\n",
    "df0 = quadratic_derivative(np.array([[0]])) # derivative at zero\n",
    "f0 = quadratic_function(np.array([[0]]))\n",
    "line_length = 0.2\n",
    "\n",
    "plt.plot(thetas, f_vals)\n",
    "plt.annotate('', xytext=(0-line_length, f0-line_length*df0), xy=(0+line_length, f0+line_length*df0),\n",
    "             arrowprops={'arrowstyle': '-', 'lw': 1.5}, va='center', ha='center')\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Objective value')\n",
    "plt.title('Simple quadratic function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pts = np.array([[0, 0.5, 0.8]]).reshape((3,1))\n",
    "df0s = quadratic_derivative(pts)\n",
    "f0s = quadratic_function(pts)\n",
    "\n",
    "plt.plot(thetas, f_vals)\n",
    "for pt, f0, df0 in zip(pts.flatten(), f0s.flatten(), df0s.flatten()): \n",
    "    plt.annotate('', xytext=(pt-line_length, f0-line_length*df0), xy=(pt+line_length, f0+line_length*df0),\n",
    "             arrowprops={'arrowstyle': '-', 'lw': 1}, va='center', ha='center')\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Objective value')\n",
    "plt.title('Simple quadratic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: Partial Derivatives\n",
    "\n",
    "The partial derivative $$\\frac{\\partial f(\\theta)}{\\partial \\theta_j}$$ of a multivariate function $f : \\mathbb{R}^d \\to \\mathbb{R}$ is the derivative of $f$ with respect to $\\theta_j$ while all other inputs $\\theta_k$ for $k\\neq j$ are fixed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: The Gradient\n",
    "\n",
    "The gradient vector $\\nabla_\\theta f$ is the generalization of the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at an arbitrary parameter vector $\\theta^*$ as\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta f (\\theta^*) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The $j$-th entry of the vector $\\nabla_\\theta f (\\theta^*)$ is the partial derivative $\\frac{\\partial f(\\theta^*)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use a quadratic function as a running example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def quadratic_function2d(theta0, theta1):\n",
    "    \"\"\"Quadratic objective function, J(theta0, theta1).\n",
    "    \n",
    "    The inputs theta0, theta1 are 2d arrays and we evaluate\n",
    "    the objective at each value theta0[i,j], theta1[i,j].\n",
    "    We implement it this way so it's easier to plot the\n",
    "    level curves of the function in 2d.\n",
    "\n",
    "    Parameters:\n",
    "    theta0 (np.array): 2d array of first parameter theta0\n",
    "    theta1 (np.array): 2d array of second parameter theta1\n",
    "    \n",
    "    Returns:\n",
    "    fvals (np.array): 2d array of objective function values\n",
    "        fvals is the same dimension as theta0 and theta1.\n",
    "        fvals[i,j] is the value at theta0[i,j] and theta1[i,j].\n",
    "    \"\"\"\n",
    "    theta0 = np.atleast_2d(np.asarray(theta0))\n",
    "    theta1 = np.atleast_2d(np.asarray(theta1))\n",
    "    return 0.5*((2*theta1-2)**2 + (theta0-3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta0_grid = np.linspace(-4,7,101)\n",
    "theta1_grid = np.linspace(-1,4,101)\n",
    "theta_grid = theta0_grid[np.newaxis,:], theta1_grid[:,np.newaxis]\n",
    "J_grid = quadratic_function2d(theta0_grid[np.newaxis,:], theta1_grid[:,np.newaxis])\n",
    "\n",
    "X, Y = np.meshgrid(theta0_grid, theta1_grid)\n",
    "contours = plt.contour(X, Y, J_grid, 10)\n",
    "plt.clabel(contours)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's write down the derivative of the quadratic function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def quadratic_derivative2d(theta0, theta1):\n",
    "    \"\"\"Derivative of quadratic objective function.\n",
    "    \n",
    "    The inputs theta0, theta1 are 1d arrays and we evaluate\n",
    "    the derivative at each value theta0[i], theta1[i].\n",
    "\n",
    "    Parameters:\n",
    "    theta0 (np.array): 1d array of first parameter theta0\n",
    "    theta1 (np.array): 1d array of second parameter theta1\n",
    "    \n",
    "    Returns:\n",
    "    grads (np.array): 2d array of partial derivatives\n",
    "        grads is of the same size as theta0 and theta1\n",
    "        along first dimension and of size\n",
    "        two along the second dimension.\n",
    "        grads[i,j] is the j-th partial derivative \n",
    "        at input theta0[i], theta1[i].\n",
    "    \"\"\"\n",
    "    # this is the gradient of 0.5*((2*theta1-2)**2 + (theta0-3)**2)\n",
    "    grads = np.stack([theta0-3, (2*theta1-2)*2], axis=1)\n",
    "    grads = grads.reshape([len(theta0), 2])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can visualize the derivative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta0_pts, theta1_pts = np.array(\n",
    "    [2.3, -1.35, -2.3]), np.array([2.4, -0.15, 2.75])\n",
    "dfs = quadratic_derivative2d(theta0_pts, theta1_pts)\n",
    "line_length = 0.2\n",
    "\n",
    "contours = plt.contour(X, Y, J_grid, 10)\n",
    "for theta0_pt, theta1_pt, df0 in zip(theta0_pts, theta1_pts, dfs):\n",
    "    plt.annotate('', xytext=(theta0_pt, theta1_pt),\n",
    "                 xy=(theta0_pt-line_length*df0[0],\n",
    "                     theta1_pt-line_length*df0[1]),\n",
    "                 arrowprops={'arrowstyle': '->', 'lw': 2}, va='center', ha='center')\n",
    "plt.scatter(theta0_pts, theta1_pts)\n",
    "plt.clabel(contours)\n",
    "plt.xlabel('Theta0')\n",
    "plt.ylabel('Theta1')\n",
    "plt.title('Gradients of the quadratic function')\n",
    "plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1b: Gradient Descent\n",
    "\n",
    "Next, we will use gradients to define an important algorithm called _gradient descent_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculus Review: The Gradient\n",
    "\n",
    "The gradient vector $\\nabla_\\theta f$ is the generalization of the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at an arbitrary parameter vector $\\theta^*$ as\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta f (\\theta^*) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The $j$-th entry of the vector $\\nabla_\\theta f (\\theta^*)$ is the partial derivative $\\frac{\\partial f(\\theta^*)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta0_pts, theta1_pts = np.array([2.3, -1.35, -2.3]), np.array([2.4, -0.15, 2.75])\n",
    "dfs = quadratic_derivative2d(theta0_pts, theta1_pts)\n",
    "line_length = 0.2\n",
    "\n",
    "contours = plt.contour(X, Y, J_grid, 10)\n",
    "for theta0_pt, theta1_pt, df0 in zip(theta0_pts, theta1_pts, dfs):\n",
    "    plt.annotate('', xytext=(theta0_pt, theta1_pt), \n",
    "                     xy=(theta0_pt-line_length*df0[0], theta1_pt-line_length*df0[1]),\n",
    "                     arrowprops={'arrowstyle': '->', 'lw': 2}, va='center', ha='center')\n",
    "plt.scatter(theta0_pts, theta1_pts)\n",
    "plt.clabel(contours)\n",
    "plt.xlabel('Theta0')\n",
    "plt.ylabel('Theta1')\n",
    "plt.title('Gradients of the quadratic function')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent Method: Intuition\n",
    "\n",
    "*Gradient descent method is  perhaps the most important optimization algorithm used in machine learning.\n",
    "The intuition behind gradient descent is the following. We know that the gradient pointing to the steepest ascent direction along which the function increases (locally). Thus to reach a minimum we simply move along the negative gradient direction. However, the gradient is a local information and thus a descent towards a minimum is guaranteed for a small step along the negative gradient direction. The gradient descent algorithm is therefore an iterative approach that continuously computes the gradient at the current point, follows the gradient descent a bit to find a new point, and then repeats the process until the gradient at the new point is sufficiently small.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent: Notation\n",
    "\n",
    "More formally, if we want to optimize $J(\\theta)$, we start with an initial guess $\\theta^0$ for the parameters and repeat the following update \n",
    "$$ \\theta^{i} := \\theta^{i-1} - \\alpha \\cdot \\nabla_\\theta J(\\theta^{i-1}). $$\n",
    "until $\\theta^i$ and $\\theta^{i-1}$  is not much different from each other or the gradient norm is sufficiently small. \n",
    "\n",
    "As code, this method may look as follows:\n",
    "\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while norm(theta - theta_prev) > convergence_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * gradient(theta_prev)\n",
    "```\n",
    "\n",
    "In the above algorithm, we stop when $||\\theta^i - \\theta^{i-1}||$ or $||\\nabla_{\\theta}(\\theta^i)||$ is small. Here the Euclidean norm $||\\theta||$ is defined as \n",
    "$$\n",
    "    ||\\theta|| = (\\sum_{i=0}^d \\theta_i^2)^{1/2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's easy to implement this function in numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "convergence_threshold = 2e-1\n",
    "step_size = 2e-1\n",
    "theta, theta_prev = np.array([[-2], [3]]), np.array([[0], [0]])\n",
    "opt_pts = [theta.flatten()]\n",
    "opt_grads = []\n",
    "\n",
    "while np.linalg.norm(theta - theta_prev) > convergence_threshold:\n",
    "    # we repeat this while the value of the function is decreasing\n",
    "    theta_prev = theta\n",
    "    gradient = quadratic_derivative2d(*theta).reshape([2,1])\n",
    "    theta = theta_prev - step_size * gradient\n",
    "    opt_pts += [theta.flatten()]\n",
    "    opt_grads += [gradient.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now visualize gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "opt_pts = np.array(opt_pts)\n",
    "opt_grads = np.array(opt_grads)\n",
    "\n",
    "contours = plt.contour(X, Y, J_grid, 10)\n",
    "plt.clabel(contours)\n",
    "plt.scatter(opt_pts[:,0], opt_pts[:,1])\n",
    "\n",
    "for opt_pt, opt_grad in zip(opt_pts, opt_grads):\n",
    "    plt.annotate('', xytext=(opt_pt[0], opt_pt[1]), \n",
    "                 xy=(opt_pt[0]-0.8*step_size*opt_grad[0], opt_pt[1]-0.8*step_size*opt_grad[1]),\n",
    "                 arrowprops={'arrowstyle': '->', 'lw': 2}, va='center', ha='center')\n",
    "\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Gradient Descent in Linear Models\n",
    "\n",
    "Let's now use gradient descent to derive a supervised learning algorithm for linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Linear Model Family\n",
    "\n",
    "Recall that a linear model has the form\n",
    "\\begin{align*}\n",
    "y & = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_d \\cdot x_d\n",
    "\\end{align*}\n",
    "where $x \\in \\mathbb{R}^d$ is a vector of features and $y$ is the target. The $\\theta_j$ are the _parameters_ of the model.\n",
    "\n",
    "By using the notation $x_0 = 1$, and thus the vector $x$ as $x = (x_0,x_1,...,x_d)^\\top$ together with $\\theta = (\\theta_0,\\theta_1,...,\\theta_d)^\\top$ we can represent the model in a vectorized form\n",
    "$$ f_\\theta(x) = \\sum_{j=0}^d \\theta_j x_j = \\theta^\\top x. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's define our model in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f(X, theta):\n",
    "    \"\"\"The linear model we are trying to fit.\n",
    "    \n",
    "    Parameters:\n",
    "    theta (np.array): d-dimensional vector of parameters\n",
    "    X (np.array): (n,d)-dimensional data matrix\n",
    "    \n",
    "    Returns:\n",
    "    y_pred (np.array): n-dimensional vector of predicted targets\n",
    "    \"\"\"\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Objective: Mean Squared Error\n",
    "\n",
    "We pick $\\theta$ to minimize the mean squared error (MSE). Slight variants of this objective are also known as the residual sum of squares (RSS) or the sum of squared residuals (SSR) or the $L^2$ loss.\n",
    "$$J(\\theta)= \\frac{1}{2n} \\sum_{i=1}^n(y^{(i)}-\\theta^\\top x^{(i)})^2$$\n",
    "In other words, we are looking for the best compromise in $\\theta$ over all the data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement mean squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(theta, X, y):\n",
    "    \"\"\"The cost function, J, describing the goodness of fit.\n",
    "    \n",
    "    Parameters:\n",
    "    theta (np.array): d-dimensional vector of parameters\n",
    "    X (np.array): (n,d)-dimensional design matrix\n",
    "    y (np.array): n-dimensional vector of targets\n",
    "    \"\"\"\n",
    "    return 0.5*np.mean((y-f(X, theta))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Squared Error: Partial Derivatives\n",
    "\n",
    "Let's work out what a partial derivative is for the MSE error loss for a linear model with a generic data pair $(x,y)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} & = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} \\left( f_\\theta(x) - y \\right)^2 \\\\\n",
    "& = \\left( f_\\theta(x) - y \\right) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( f_\\theta(x) - y \\right) \\\\\n",
    "& = \\left( f_\\theta(x) - y \\right) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( \\sum_{k=0}^d \\theta_k \\cdot x_k - y \\right) \\\\\n",
    "& = \\left( f_\\theta(x) - y \\right) \\cdot x_j\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Squared Error: The Gradient\n",
    "\n",
    "We can use this derivation to obtain an expression for the gradient of the MSE for a linear model\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J (\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\left( f_\\theta(x) - y \\right) \\cdot x_0 \\\\\n",
    "\\left( f_\\theta(x) - y \\right) \\cdot x_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\left( f_\\theta(x) - y \\right) \\cdot x_d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\left( f_\\theta(x) - y \\right) \\cdot x\n",
    ".\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mse_gradient(theta, X, y):\n",
    "    \"\"\"The gradient of the cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    theta (np.array): d-dimensional vector of parameters\n",
    "    X (np.array): (n,d)-dimensional design matrix\n",
    "    y (np.array): n-dimensional vector of targets\n",
    "    \n",
    "    Returns:\n",
    "    grad (np.array): d-dimensional gradient of the MSE\n",
    "    \"\"\"\n",
    "    return np.mean((f(X, theta) - y) * X.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The UCI Diabetes Dataset\n",
    "\n",
    "In this section, we are going to again use the UCI Diabetes Dataset.\n",
    "\n",
    "- For each patient we have a access to a measurement of their body mass index (BMI) and a quantiative diabetes risk score (from 0-300).\n",
    "- We are interested in understanding how BMI affects an individual's diabetes risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "# add an extra column of onens\n",
    "X['one'] = 1\n",
    "\n",
    "# Collect 20 data points and only use bmi dimension\n",
    "X_train = X.iloc[-20:].loc[:, ['bmi', 'one']]\n",
    "y_train = y.iloc[-20:] / 300\n",
    "\n",
    "plt.scatter(X_train.loc[:,['bmi']], y_train,  color='black')\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent for Linear Regression\n",
    "\n",
    "Putting this together with the gradient descent algorithm, we obtain a learning method for training linear models.\n",
    "\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while abs(J(theta) - J(theta_prev)) > conv_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * (f(x, theta)-y) * x\n",
    "```\n",
    "\n",
    "This update rule is also known as the Least Mean Squares (LMS) or Widrow-Hoff learning rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 1e-3\n",
    "step_size = 4e-1\n",
    "theta, theta_prev = np.array([2,1]), np.ones(2,)\n",
    "opt_pts = [theta]\n",
    "opt_grads = []\n",
    "iter = 0\n",
    "\n",
    "while np.linalg.norm(theta - theta_prev) > threshold:\n",
    "    if iter % 100 == 0:\n",
    "        print('Iteration %d. MSE: %.6f' % (iter, mean_squared_error(theta, X_train, y_train)))\n",
    "    theta_prev = theta\n",
    "    gradient = mse_gradient(theta, X_train, y_train)\n",
    "    theta = theta_prev - step_size * gradient\n",
    "    opt_pts += [theta]\n",
    "    opt_grads += [gradient]\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_line = np.stack([np.linspace(-0.1, 0.1, 10), np.ones(10,)])\n",
    "y_line = opt_pts[-1].dot(x_line)\n",
    "\n",
    "plt.scatter(X_train.loc[:,['bmi']], y_train,  color='black')\n",
    "plt.plot(x_line[0], y_line)\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3: Ordinary Least Squares\n",
    "\n",
    "In practice, there is a more effective way than gradient descent to find linear model parameters.\n",
    "\n",
    "We will see this method here, which will lead to our first non-toy algorithm: Ordinary Least Squares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: The Gradient\n",
    "Recall the gradient vector $\\nabla_\\theta f$ is the generalization of the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at an arbitrary parameter vector $\\theta^*$ as\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta f (\\theta^*) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta^*)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The $j$-th entry of the vector $\\nabla_\\theta f (\\theta^*)$ is the partial derivative $\\frac{\\partial f(\\theta^*)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The UCI Diabetes Dataset\n",
    "\n",
    "In this section, we are going to again use the UCI Diabetes Dataset.\n",
    "\n",
    "- For each patient we have a access to a measurement of their body mass index (BMI) and a quantiative diabetes risk score (from 0-300).\n",
    "- We are interested in understanding how BMI affects an individual's diabetes risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "# add an extra column of onens\n",
    "X['one'] = 1\n",
    "\n",
    "# Collect 20 data points\n",
    "X_train = X.iloc[-20:]\n",
    "y_train = y.iloc[-20:]\n",
    "\n",
    "plt.scatter(X_train.loc[:,['bmi']], y_train,  color='black')\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation: Design Matrix\n",
    "\n",
    "<!-- Suppose that we have a dataset of size $n$ (e.g., $n$ patients), indexed by $i=1,2,...,n$. Each $x^{(i)}$ is a vector of $d$ features. -->\n",
    "\n",
    "Machine learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix $X \\in \\mathbb{R}^{n \\times d}$, of the form:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\ldots & x^{(1)}_d \\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\ldots & x^{(2)}_d \\\\\n",
    "\\vdots \\\\\n",
    "x^{(n)}_1 & x^{(n)}_2 & \\ldots & x^{(n)}_d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "- & (x^{(1)})^\\top & - \\\\\n",
    "- & (x^{(2)})^\\top & - \\\\\n",
    "& \\vdots & \\\\\n",
    "- & (x^{(n)})^\\top & - \\\\\n",
    "\\end{bmatrix}\n",
    ".\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can view the design matrix for the diabetes dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation: Design Matrix\n",
    "\n",
    "Similarly, we can vectorize the target variables into a vector $y \\in \\mathbb{R}^n$ of the form\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(n)}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Squared Error in Matrix Form\n",
    "\n",
    "Recall that we may fit a linear model by choosing $\\theta$ that minimizes the squared error:\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^\\top x^{(i)})^2$$\n",
    "In other words, we are looking for the best compromise in $\\theta$ over all the data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can write this sum in matrix-vector form as:\n",
    "$$J(\\theta) = \\frac{1}{2} (y-X\\theta)^\\top(y-X\\theta) = \\frac{1}{2} \\|y-X\\theta\\|^2,$$\n",
    "where $X$ is the design matrix and $\\|\\cdot\\|$ denotes the Euclidean norm, again, defined as\n",
    "$$\n",
    "    ||y|| = (\\sum_{i=1}^n y_i^2)^{1/2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Gradient of the Squared Error\n",
    "\n",
    "We can a gradient for the mean squared error as follows.\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla*\\theta J(\\theta)\n",
    "& = \\nabla_\\theta \\frac{1}{2} (X \\theta - y)^\\top (X \\theta - y) \\\\\n",
    "& = \\frac{1}{2} \\nabla_\\theta \\left( (X \\theta)^\\top (X \\theta) - (X \\theta)^\\top y - y^\\top (X \\theta) + y^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\nabla_\\theta \\left( \\theta^\\top (X^\\top X) \\theta - 2(X \\theta)^\\top y \\right) \\\\\n",
    "& = \\frac{1}{2} \\left( 2(X^\\top X) \\theta - 2X^\\top y \\right) \\\\\n",
    "& = (X^\\top X) \\theta - X^\\top y\n",
    "\\end{align*}\n",
    "\n",
    "We used the facts that $a^\\top b = b^\\top a$ (line 3), that $\\nabla_x b^\\top x = b$ (line 4), and that $\\nabla_x x^\\top A x = 2 A x$ for a symmetric matrix $A$ (line 4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normal Equations\n",
    "\n",
    "<!-- We know from calculus that a function is minimized when its derivative is set to zero. In our case, our objective function is a (multivariate) quadratic; hence it only has one minimum, which is the global minimum.\n",
    " -->\n",
    "\n",
    "Setting the above derivative to zero, we obtain the _normal equations_:\n",
    "$$ (X^\\top X) \\theta = X^\\top y.$$\n",
    "\n",
    "Hence, the value $\\theta^*$ that minimizes this objective is given by:\n",
    "$$ \\theta^* = (X^\\top X)^{-1} X^\\top y.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that we assumed that the matrix $(X^\\top X)$ is invertible; if this is not the case, there are easy ways of addressing this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's apply the normal equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta_best = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "theta_best_df = pd.DataFrame(data=theta_best[np.newaxis, :], columns=X.columns)\n",
    "theta_best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now use our estimate of theta to compute predictions for 3 new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Collect 3 data points for testing\n",
    "X_test = X.iloc[:3]\n",
    "y_test = y.iloc[:3]\n",
    "\n",
    "# generate predictions on the new patients\n",
    "y_test_pred = X_test.dot(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize these predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')\n",
    "plt.scatter(X_train.loc[:, ['bmi']], y_train)\n",
    "plt.scatter(X_test.loc[:, ['bmi']], y_test, color='red', marker='o')\n",
    "plt.plot(X_test.loc[:, ['bmi']], y_test_pred, 'x', color='red', mew=3, markersize=8)\n",
    "plt.legend(['Training', 'Testing','Prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Ordinary Least Squares\n",
    "\n",
    "- **Type**: Supervised learning (regression)\n",
    "- **Model family**: Linear models\n",
    "- **Objective function**: Mean squared error\n",
    "- **Optimizer**: Normal equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 4: Non-Linear Least Squares\n",
    "\n",
    "So far, we have learned about a very simple linear model. These can capture only simple linear relationships in the data. How can we use what we learned so far to model more complex relationships?\n",
    "\n",
    "We will now see a simple approach to model complex non-linear relationships called _least squares_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Polynomial Functions\n",
    "\n",
    "Recall that a polynomial of degree $p$ is a function of the form\n",
    "\n",
    "$$\n",
    "a_p x^p + a_{p-1} x^{p-1} + ... + a_{1} x + a_0.\n",
    "$$\n",
    "\n",
    "Below are some examples of polynomial functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "x_vars = np.linspace(-2, 2)\n",
    "\n",
    "plt.subplot('131')\n",
    "plt.title('Quadratic Function')\n",
    "plt.plot(x_vars, x_vars**2)\n",
    "plt.legend([\"$x^2$\"])\n",
    "\n",
    "plt.subplot('132')\n",
    "plt.title('Cubic Function')\n",
    "plt.plot(x_vars, x_vars**3)\n",
    "plt.legend([\"$x^3$\"])\n",
    "\n",
    "plt.subplot('133')\n",
    "plt.title('Third Degree Polynomial')\n",
    "plt.plot(x_vars, x_vars**3 + 2*x_vars**2 + x_vars + 1)\n",
    "plt.legend([\"$x^3 + 2 x^2 + x + 1$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling Non-Linear Relationships With Polynomial Regression\n",
    "\n",
    "<!-- Note that the set of $p$-th degree polynomials forms a linear model with parameters $a_p, a_{p-1}, ..., a_0$.\n",
    "This means we can use our algorithms for linear models to learn non-linear features! -->\n",
    "\n",
    "Specifically, given a one-dimensional continuous variable $x$, we can defining a feature function $\\phi : \\mathbb{R} \\to \\mathbb{R}^{p+1}$ as\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "x^2 \\\\\n",
    "\\vdots \\\\\n",
    "x^p\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The class of models of the form\n",
    "$$ f_\\theta(x) := \\sum_{j=0}^p \\theta_j \\phi_j = \\theta^\\top \\phi(x) $$\n",
    "with parameters $\\theta$ and polynomial features $\\phi$ is the set of $p$-degree polynomials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This model is non-linear in the input variable $x$, meaning that we can model complex data relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is a linear model as a function of the parameters $\\theta$, meaning that we can use our familiar ordinary least squares algorithm to learn these features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The UCI Diabetes Dataset\n",
    "\n",
    "In this section, we are going to again use the UCI Diabetes Dataset.\n",
    "\n",
    "- For each patient we have a access to a measurement of their body mass index (BMI) and a quantiative diabetes risk score (from 0-300).\n",
    "- We are interested in understanding how BMI affects an individual's diabetes risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "# add an extra column of onens\n",
    "X['one'] = 1\n",
    "\n",
    "# Collect 20 data points\n",
    "X_train = X.iloc[-20:]\n",
    "y_train = y.iloc[-20:]\n",
    "\n",
    "plt.scatter(X_train.loc[:,['bmi']], y_train,  color='black')\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diabetes Dataset: A Non-Linear Featurization\n",
    "\n",
    "Let's now obtain linear features for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_bmi = X_train.loc[:, ['bmi']]\n",
    "\n",
    "X_bmi_p3 = pd.concat([X_bmi, X_bmi**2, X_bmi**3], axis=1)\n",
    "X_bmi_p3.columns = ['bmi', 'bmi2', 'bmi3']\n",
    "X_bmi_p3['one'] = 1\n",
    "X_bmi_p3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diabetes Dataset: A Polynomial Model\n",
    "\n",
    "By training a linear model on this featurization of the diabetes set, we can obtain a polynomial model of diabetest risk as a function of BMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression\n",
    "theta = np.linalg.inv(X_bmi_p3.T.dot(X_bmi_p3)).dot(X_bmi_p3.T).dot(y_train)\n",
    "\n",
    "# Show the learned polynomial curve\n",
    "x_line = np.linspace(-0.1, 0.1, 10)\n",
    "x_line_p3 = np.stack([x_line, x_line**2, x_line**3, np.ones(10,)], axis=1)\n",
    "y_train_pred = x_line_p3.dot(theta)\n",
    "\n",
    "plt.xlabel('Body Mass Index (BMI)')\n",
    "plt.ylabel('Diabetes Risk')\n",
    "plt.scatter(X_bmi, y_train)\n",
    "plt.plot(x_line, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Polynomial Regression\n",
    "\n",
    "We can also take this approach to construct non-linear function of multiples variable by using multivariate polynomials.\n",
    "\n",
    "For example, a polynomial of degree $2$ over two variables $x_1, x_2$ is a function of the form\n",
    "\n",
    "<!-- $$\n",
    "a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{22} x_1^2 x_2^2 + a_{21} x_1^2 x_2 + a_{12} x_1 x_2^2 + a_11 x_1 x_2 + a_{00}.\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "a_{20} x_1^2 + a_{10} x_1 + a_{02} x_2^2 + a_{01} x_2 + a_{11} x_1 x_2 + a_{00}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, a polynomial of degree $p$ over two variables $x_1, x_2$ is a function of the form\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = \\sum_{i,j \\geq 0 : i+j \\leq p} a_{ij} x_1^i x_2^j.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our two-dimensional example, this corresponds to a feature function $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}^6$ of the form\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_1^2 \\\\\n",
    "x_2 \\\\\n",
    "x_2^2 \\\\\n",
    "x_1 x_2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The same approach holds for polynomials of an degree and any number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Towards General Non-Linear Features\n",
    "\n",
    "Any non-linear feature map $\\phi(x) : \\mathbb{R}^d \\to \\mathbb{R}^p$ can be used in this way to obtain general models of the form\n",
    "$$ f_\\theta(x) := \\theta^\\top \\phi(x) $$\n",
    "that are highly non-linear in $x$ but linear in $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, here is a way of modeling complex periodic functions via a sum of sines and cosines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "x_vars = np.linspace(-5, 5)\n",
    "\n",
    "plt.subplot('131')\n",
    "plt.title('Cosine Function')\n",
    "plt.plot(x_vars, np.cos(x_vars))\n",
    "plt.legend([\"$cos(x)$\"])\n",
    "\n",
    "plt.subplot('132')\n",
    "plt.title('Sine Function')\n",
    "plt.plot(x_vars, np.sin(2*x_vars))\n",
    "plt.legend([\"$x^3$\"])\n",
    "\n",
    "plt.subplot('133')\n",
    "plt.title('Combination of Sines and Cosines')\n",
    "plt.plot(x_vars, np.cos(x_vars) + np.sin(2*x_vars) + np.cos(4*x_vars))\n",
    "plt.legend([\"$cos(x) + sin(2x) + cos(4x)$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Non-Linear Least Squares\n",
    "\n",
    "- **Type**: Supervised learning (regression)\n",
    "- **Model family**: Linear in the parameters; non-linear with respect to raw inputs.\n",
    "- **Features**: Non-linear functions of the attributes\n",
    "- **Objective function**: Mean squared error\n",
    "- **Optimizer**: Normal equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-ode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('skitlearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "rise": {
   "controlsTutorial": false,
   "height": 900,
   "help": false,
   "margin": 0,
   "maxScale": 2,
   "minScale": 0.2,
   "progress": true,
   "scroll": true,
   "theme": "simple",
   "width": 1200
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
