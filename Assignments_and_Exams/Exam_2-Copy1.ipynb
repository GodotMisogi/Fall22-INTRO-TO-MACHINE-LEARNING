{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Exam 2</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Available: 7am-7pm, Friday, 28 October.</center></h1>\n",
    "##### <h1><center>Available support: 10am-1:00:00pm, Friday, 28 October.</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (+25)\n",
    "\n",
    "Discuss the difference and point out the pros and cons of generative and discriminative model. Give an example for each type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2** (+25) (Maximum Likelihood for Multiple outputs linear regression problem)\n",
    "We consider the linear regression with $M$ multiple outputs, i.e., \n",
    "\n",
    "$$y = W^T \\phi(x),$$\n",
    "\n",
    "where $W \\in \\mathcal{R}^{d \\times M} $ is the paramter matrix, $\\phi(x)$ is nonlinear feature map (x is a scalar in this problem). Assume that outputs $y_j$ are independent, the model for each output is\n",
    "\n",
    "$$ p  \\left( y_j^i | x^i, w_j \\right) = \\mathcal{N} \\left( y_j^i | w_j^T \\phi(x^i), \\sigma_j^2 \\right) $$\n",
    "\n",
    "and then the full model reads  \n",
    "\n",
    "$$ p  \\left( y^i| x^i, W \\right) = \\prod_{j=1}^M \\mathcal{N} \\left( y_j^i | w_j^T \\phi(x^i), \\sigma_j^2 \\right), $$\n",
    "\n",
    "where $i$ is training sample index, $j$ is j-th component of output $y$, $w_j \\in \\mathcal{R}^{d \\times 1} $ is j-th column of matrix $W$. If $M = 1$ then we have linear regression problem with one output.\n",
    "\n",
    "1. Finding maximum likelihood estimation (MLE) solution for $W$ from given N samples training data set, $(\\phi(x^i), y^i)_{i=1}^N$.\n",
    "\n",
    "2. Assume the training data (N = 6) is given as in the following table,\n",
    "\n",
    "\\begin{array}{c | c}\n",
    "x & y\\\\\n",
    "\\hline\n",
    "0 & (-1,-1) \\\\\n",
    "0 & (-1,-2) \\\\\n",
    "0 & (-2,-1) \\\\\n",
    "1 & (1,1) \\\\\n",
    "1 & (1,2) \\\\\n",
    "1 & (2,1) \n",
    "\\end{array}\n",
    "\n",
    "and the transformed feature matrix ($d=2$) is\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi(0) = (0,1) \\\\\n",
    "\\phi(1) = (1,0) \n",
    "\\end{align*}\n",
    "\n",
    "Compute the $W$ by the formula obtained in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (+25) (Gaussian Generative vs. Discriminative Model Classes)\n",
    "\n",
    "In binary classification, we saidthat the conditional probability $P_\\theta(y|x)$ of a LDA model has the form\n",
    "$$ P_\\theta(y|x) = \\frac{P_\\theta(x|y)P_\\theta(y)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y')P_\\theta(y')} = \\frac{1}{1+\\exp(-\\gamma^\\top x)} $$\n",
    "for some set of parameters $\\gamma$ (whose expression can be derived from $\\theta$). Thus, LDG is a Logistic Regression! \n",
    "\n",
    "In this problem we set out to verify this fact.\n",
    "\n",
    "Let $P_\\theta[y_1] = \\phi_1, P_\\theta[y_2] = \\phi_2$ and $P_\\theta[x|y = y_1] = \\mathcal{N}(x| \\mu_1, \\Sigma), P_\\theta[x|y = y_2] = \\mathcal{N}(x| \\mu_2, \\Sigma)$, show that\n",
    "\n",
    "$$ P_\\theta(y=y_2|x) = \\frac{P_\\theta(x|y = y_2)P_\\theta(y = y_2)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y = y')P_\\theta(y = y')}  = \\frac{1}{1+\\exp(-\\theta_0-\\gamma^\\top x)} $$\n",
    "\n",
    "for some set of parameter vector $\\gamma$ and a scalar $\\theta_0$. Determine $\\gamma$ and $\\theta_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4** (+25) (Probabilistic Approach to Classification)\n",
    "\n",
    "We use a logistic model to parametrize a probability distribution as follows:\n",
    "\\begin{align*}\n",
    "p(y=0 | x;\\theta) & = \\sigma(\\theta^\\top x) \\\\\n",
    "p(y=1 | x;\\theta) & = 1-\\sigma(\\theta^\\top x),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function. \n",
    "\n",
    "1. Derive the loglikelihood for the logistic regression.\n",
    "2. Derive the gradient of the loglikehood for a pair of generic data $(x,y)$\n",
    "3. Suppose that we have found the best $\\theta^*$ by maximum likelihood. If we assign a label 0 for a new sample $\\tilde{x}$ when $p(\\tilde{y}=1 | \\tilde{x};\\theta^*) < 0.5$, and 1 otherwise. Show that the decision boundary is a hyperplane (linear).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 5** (bonous +5) (Elastic net regularization is equivalent to Lasso regularization)\n",
    "In assignment 1, we compare different common regularization approaches to solve overfitting issue. We have the Elastic net loss function is\n",
    "\n",
    "$$\n",
    "J_1(w) = |Y - Xw|_2^2 + \\lambda_1 |w|_2^2 + \\lambda_2 |w|_1\n",
    "$$\n",
    "\n",
    "where $|.|_2$ means l2-norm and $|.|_1$ means l1-norm, $X \\in \\mathcal{R}^{N\\times d}$, $Y \\in \\mathcal{R}^{N\\times 1}$, $w \\in \\mathcal{R}^{d\\times 1}$. \n",
    "\n",
    "**The goal of this problem is to solve that an elastic net problem is equivalent to a Lasso problem**.\n",
    "\n",
    "Show that minimizing the elastic net loss function is equivalent to minimizing the following loss function \n",
    "\n",
    "$$\n",
    "J_2(\\tilde{w}) = |\\tilde{Y} - \\tilde{X}\\tilde{w}|_2^2 + \\lambda_1 |\\tilde{w}|_1\n",
    "$$\n",
    "\n",
    "for some $\\tilde{Y}$, $ \\tilde{X}$, and $\\tilde{w}$. Determine $\\tilde{Y}$, $ \\tilde{X}$, and $\\tilde{w}$ in terms of quantities defined in $J_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 6** (bonous +5) (beta-binomial model):\n",
    "Flipping a coin problem is a example of Bernouli distribution (a special case of Binomial distribution). And, beta distribution is a \"conjugate prior\" for Binomial distribution. We denote $\\#\\,\\text{heads}$ in the number of times of getting heads and $\\#\\,\\text{tails}$ is number of times of getting tails. In the lecutre 5, we assume that \n",
    "\n",
    "$$P[x = H] = \\theta, \\quad P[x = T] = 1-\\theta, $$\n",
    "and the prior for $\\theta$ is given by\n",
    "$$ P(\\theta) = \\text{Beta}(x,|\\alpha,\\beta), $$\n",
    "where $\\alpha, \\beta >0$ are parameters and $B$ is the Beta function. Then, we have derived the mean of the posterior distribution in $(\\#\\,\\text{heads} + \\#\\,\\text{tails})$ trials:\n",
    "$$\\theta^*= \\frac{\\#\\,\\text{heads} + \\alpha}{\\#\\,\\text{heads}+\\#\\,\\text{tails} + \\alpha + \\beta}$$\n",
    "\n",
    "1. Finding the variance of posterior distribution. Discuss the relationship between the variance and the number of training samples.\n",
    "\n",
    "2. Finding the posterior predictive distribution of getting Head from a single trial, i.e,\n",
    "$$P[x = H|\\mathcal{D}] = \\int_0^1 P[x = H|\\theta] \\  P[\\theta|\\mathcal{D}] d\\theta,$$\n",
    "\n",
    "**Note that we use $H$ interchangeably with $1$ and $T$ interchangeable with $0$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
