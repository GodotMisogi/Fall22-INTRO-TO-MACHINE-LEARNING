{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Assigment 4</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Due day: 11:00 pm, 2022 </center></h1>\n",
    "#### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Probalistic linear regression )\n",
    "In assignment 3, question 3. We have computed the posterior distribution by using $\\theta^*$ (the MAP point) to achieve the mean, and keep the variance the same as the noise level $\\delta^2$. Instead of that, using Bayesian Predictions, finding the mean and variance of \n",
    "\n",
    "$$ P(y | x, \\mathcal{D}) = \\int_\\theta P(y \\mid x, \\theta) P(\\theta \\mid \\mathcal{D}) d\\theta. $$\n",
    "\n",
    "1. Plotting the the uncertainty band.\n",
    "2. Discuss the result by comparing with the results that you achieved in assignment 3 (just using the MAP point).\n",
    "\n",
    "## **Question 2**\n",
    "\n",
    "1. In lecture 7, we can maximize the likelihood \n",
    "$$\\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log P(x^{(i)} | y^{(i)} ; \\mu_k, \\Sigma_k)\n",
    "= \\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log \\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)$$\n",
    "over the Gaussian parameters. Prove that the empirical means and covariances of each class are\n",
    "\\begin{align*}\n",
    "\\mu_k & = \\frac{\\sum_{i: y^{(i)} = k} x^{(i)}}{n_k} \\\\\n",
    "\\Sigma_k & = \\frac{\\sum_{i: y^{(i)} = k} (x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^\\top}{n_k}.\n",
    "\\end{align*}\n",
    "\n",
    "2. (bonous 10+) In lecture 7, using $P_\\theta(y = i) = \\phi_i$ requires the constraint $\\sum_{k = 1}^K\\phi_k = 1$. Using Lagrangian multipliers method shows that $\\theta_k = \\frac{n_k}{n}$. (Please see Spring_21__Lecture_Notes_Math_II.pdf/section 3.6.2 in the suplementary material.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: (Generative VS Discriminative Models)\n",
    "\n",
    "We are going to use the Iris flower dataset. We will use three features sepal length (cm), sepal width (cm), petal length (cm) to classify types of flowers in generative model.\n",
    "1. Compute the emprical means and covariance matrices for each type of flowers.\n",
    "2. Generating new flowers, denoted as set $S$, from the learned propability and plotting these generated flowers in 3D. Observe and discuss your results.\n",
    "3. We assume that set $S$ is test data. Classify the test data $S$ using the generative model that you have found in question 1. Discuss you results.\n",
    "4. We assume that set $S$ is test data. Using logistic regression to training the model, then verify the learn model with test data $S$. Compare and discuss your results with question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: (Bernoulli Naive Bayes Model)\n",
    "\n",
    "1. From the lecture 8, we have the formula for the optimal value of $\\psi_{jk}$\n",
    "\\begin{align*}\n",
    "\\psi_{jk} = \\frac{n_{jk}}{n_k}.\n",
    "\\end{align*}\n",
    "Derive this formula.\n",
    "2. CAN YOU PUT A QUESTION ON CLASSIFYING A TEXT HERE WITH BERNOULLI NAIVE BAYES BASED ON THE DEMONSTRATING PROBLEM IN LECTURE 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4** (beta-binomial model): HAI, THIS QUESTION IS GREAT, BUT NOT RELEVANT FOR THE CURRENT LECTURE. WE CAN SAVE IT FOR THE EXAM\n",
    "Flipping a coin problem is a example of Bernouli distribution (a special case of Binomial distribution). And, beta distribution is a \"conjugate prior\" for Binomial distribution. In the lecutre 5, we have got the mean of the posterior distribution in $(\\#\\,\\text{heads} + \\#\\,\\text{tails})$ trial:\n",
    "$$\\theta^*= \\frac{\\#\\,\\text{heads} + \\alpha}{\\#\\,\\text{heads}+\\#\\,\\text{tails} + \\alpha + \\beta}$$\n",
    "\n",
    "1. Finding the covariance of posterior distribution (Hint: convert to beta distribution). Discuss the relationship between the covariance and the number of samples.\n",
    "2. Finding the posterior preidctive distribution of getting one Head from one trial, i.e,\n",
    "$$p[x = 1 \\text{Heads}|\\mathcal{D}] = \\int_0^1 p[x = 1 \\text{Head}|\\theta] \\  p[\\theta|\\mathcal{D}] d\\theta$$\n",
    "\n",
    "Recall: beta distribution\n",
    "\n",
    "$$\n",
    "\\text{Beta}(x,|a,b) = \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1},\n",
    "$$\n",
    "where $B(a,b)$ is a normalized constant. The mean and the variance are\n",
    "\n",
    "$$ \\text{mean} = \\frac{a}{a+b}, \\quad \\text{var} = \\frac{ab}{(a+b)^2(a+b+1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
