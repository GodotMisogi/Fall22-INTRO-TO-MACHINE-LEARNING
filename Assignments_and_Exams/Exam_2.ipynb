{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Exam 2</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Available: 7am-7pm, Friday, 28 October.</center></h1>\n",
    "##### <h1><center>Available support: 10am-1:00:00pm, Friday, 28 October.</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Elastic net regularization is equivalent to Lasso regularization)\n",
    "In assignment 1, we compare different common regularization approaches to solve overfitting issue. We have the Elastic net loss function is\n",
    "\n",
    "$$\n",
    "J_1(w) = |Y - Xw|_2^2 + \\lambda_1 |w|_2^2 + \\lambda_2 |w|_1\n",
    "$$\n",
    "\n",
    "where $|.|_2$ means l2-norm and $|.|_1$ means l1-norm, $X \\in \\mathcal{R}^{N\\times d}$, $Y \\in \\mathcal{R}^{N\\times 1}$, $w \\in \\mathcal{R}^{d\\times 1}$. Now, we define a modified lasso regularization loss function\n",
    "\n",
    "$$\n",
    "J_2(\\tilde{w}) = |\\tilde{Y} - \\tilde{X}\\tilde{w}|_2^2 + c \\lambda_1 |\\tilde{w}|_1\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \\tilde{X} = c \\begin{pmatrix} X \\\\ \\sqrt{\\lambda_2} I_{d \\times d} \\end{pmatrix}, \\quad \\tilde{y} = \\begin{pmatrix} y \\\\ 0_{d \\times 1} \\end{pmatrix}, \\quad \\tilde{w} \\in \\mathcal{R}^{d\\times 1}. $$\n",
    "\n",
    "Show that \n",
    "\n",
    "$$ \\min_{w} J_1(w) = c \\min_{\\tilde{w}} J_2(\\tilde{w})$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$J_1(cw) = J_2(w).$$ \n",
    "\n",
    "Hence, one can solve an elastic net problem using a lasso solver on modified data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2** (Maximum Likelihood for Multiple outputs linear regression problem)\n",
    "We consider the linear regression with $M$ multiple outputs, i.e., \n",
    "\n",
    "$$y = W^T \\phi(x),$$\n",
    "\n",
    "where $W \\in \\mathcal{R}^{d \\times M} $ is the paramter matrix, $\\phi(x)$ is nonlinear mapping function of feature x (x is a scalar in this problem). Assume that outputs $y_j$ are independent, the model for each output is\n",
    "\n",
    "$$ p  \\left( y_j^i | x^i, w_j \\right) = \\mathcal{N} \\left( y_j^i | w_j^T \\phi(x^i), \\sigma_j^2 \\right) $$\n",
    "\n",
    "and then the full model reads  \n",
    "\n",
    "$$ p  \\left( y^i| x^i, W \\right) = \\prod_{j=1}^M \\mathcal{N} \\left( y_j^i | w_j^T \\phi(x^i), \\sigma_j^2 \\right), $$\n",
    "\n",
    "where $i$ is training sample index, $j$ is j-th component of output $y$, $w_j \\in \\mathcal{R}^{d \\times 1} $ is j-th column of matrix $W$. If $M = 1$ then we have linear regression problem with one output.\n",
    "\n",
    "1. Finding maximum likelihood estimation (MLE) solution for $W$ from given N samples training data set, $(\\phi(x^i), y^i)_{i=1}^N$.\n",
    "\n",
    "2. Assume the training data (N = 6) is given as in the following table,\n",
    "\n",
    "\\begin{array}{c | c}\n",
    "x & y\\\\\n",
    "\\hline\n",
    "0 & (-1,-1) \\\\\n",
    "0 & (-1,-2) \\\\\n",
    "0 & (-2,-1) \\\\\n",
    "1 & (1,1) \\\\\n",
    "1 & (1,2) \\\\\n",
    "1 & (2,1) \n",
    "\\end{array}\n",
    "\n",
    "and the transformed feature matrix ($d=2$) is\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi(0) = (1,0) \\\\\n",
    "\\phi(1) = (0,1) \n",
    "\\end{align*}\n",
    "\n",
    "Compute the $W$ by the formula obtained in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (beta-binomial model):\n",
    "Flipping a coin problem is a example of Bernouli distribution (a special case of Binomial distribution). And, beta distribution is a \"conjugate prior\" for Binomial distribution. We denote $\\#\\,\\text{heads}$ in the number of times of getting heads and $\\#\\,\\text{tails}$ is number of times of getting tails. In the lecutre 5, we assume that \n",
    "\n",
    "$$P[x = H] = \\theta, \\quad P[x = T] = 1-\\theta, $$\n",
    "and \n",
    "$$ P(\\theta) = \\text{Beta}(x,|\\alpha,\\beta), $$\n",
    "where $\\alpha, \\beta >0$ are parameters and $B$ is the Beta function. Then, we have derived the mean of the posterior distribution in $(\\#\\,\\text{heads} + \\#\\,\\text{tails})$ trials:\n",
    "$$\\theta^*= \\frac{\\#\\,\\text{heads} + \\alpha}{\\#\\,\\text{heads}+\\#\\,\\text{tails} + \\alpha + \\beta}$$\n",
    "\n",
    "1. Finding the variance of posterior distribution (Hint: convert to beta distribution). Discuss the relationship between the variance and the number of training samples.\n",
    "\n",
    "2. Finding the posterior preidctive distribution of getting Head from a single trial, i.e,\n",
    "$$P[x = H|\\mathcal{D}] = \\int_0^1 P[x = H|\\theta] \\  P[\\theta|\\mathcal{D}] d\\theta,$$\n",
    "\n",
    "where we $P[x=1]$ is the propability getting Head.\n",
    "\n",
    "Recall: beta distribution\n",
    "\n",
    "$$\n",
    "\\text{Beta}(x,|\\alpha,\\beta) = \\frac{1}{B(\\alpha+1,\\beta+1)} x^{\\alpha} (1-x)^{\\beta},\n",
    "$$\n",
    "where $B(\\alpha +1 ,\\beta + 1)$ is a normalized constant. The mean and the variance are\n",
    "\n",
    "$$ \\text{mean} = \\frac{\\alpha}{\\alpha+\\beta}, \\quad \\text{var} = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4** \n",
    "\n",
    "Discuss the difference and point out the pros and cons of generative and discriminative model. Give an example for each type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 5** (Gaussian Generative vs. Discriminative Model Classes)\n",
    "\n",
    "In binary classification, we can also show that the conditional probability $P_\\theta(y|x)$ of a Gaussian Naive Bayes (**replacing Bernoulli with Gaussian when $x$ is continuous**) or LDA model has the form\n",
    "$$ P_\\theta(y|x) = \\frac{P_\\theta(x|y)P_\\theta(y)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y')P_\\theta(y')} = \\frac{1}{1+\\exp(-\\gamma^\\top x)} $$\n",
    "for some set of parameters $\\gamma$ (whose expression can be derived from $\\theta$), which is the same form as Logistic Regression!\n",
    "\n",
    "Question: Let $P_\\theta[y_1] = \\phi_1, P_\\theta[y_2] = \\phi_2$ and $P_\\theta[x|y = y_1] = \\mathcal{N}(x| \\mu_1, \\Sigma), P_\\theta[x|y = y_2] = \\mathcal{N}(x| \\mu_2, \\Sigma)$, show that (the last equality)\n",
    "\n",
    "$$ P_\\theta(y=1|x) = \\frac{P_\\theta(x|y = y_1)P_\\theta(y = y_1)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y = y')P_\\theta(y = y')} = \\frac{ \\mathcal{N}(x| \\mu_1, \\Sigma)\\phi_1}{ \\mathcal{N}(x| \\mu_1, \\Sigma)\\phi_1 +  \\mathcal{N}(x| \\mu_2, \\Sigma) \\phi_2} = \\frac{1}{1+\\exp(-\\gamma^\\top x)} $$\n",
    "\n",
    "for some set of parameters $\\gamma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 6** (Probabilistic Approach to Classification)\n",
    "\n",
    "We use logistic model to parametrize a probability distribution as follows:\n",
    "\\begin{align*}\n",
    "p(y=1 | x;\\theta) & = \\sigma(\\theta^\\top x) \\\\\n",
    "p(y=0 | x;\\theta) & = 1-\\sigma(\\theta^\\top x),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function. Show that:\n",
    "\n",
    "if we classify by $p(y=1 | x;\\theta^*)  = \\sigma({\\theta^*}^\\top x) < 0.5$, the predicted label is $1$, otherwise $0$, then the decision boundary depends on linearly on $x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
