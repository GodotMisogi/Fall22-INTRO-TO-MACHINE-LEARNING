{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Assigment 3</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Due day: 11:00 pm, Thursday, 06 October, 2022 </center></h1>\n",
    "#### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Probability + random variables)\n",
    "\n",
    "Consider $\\Omega = [-2, 2]$ and define \n",
    "$$P[B \\subset \\Omega] := \\int_{B} \\frac{1}{4} d \\omega, \\text{ and thus }\n",
    "P[d\\omega] := \\frac{1}{4} d \\omega,\n",
    "$$\n",
    "that is the elementary event $\\omega$ is the uniform random variable on $\\Omega$.\n",
    "Now, define $M(\\omega)$ as: \n",
    "\\begin{align}\n",
    "    &M(\\omega) =\n",
    "    \\begin{cases} \n",
    "        2 & \\text{ if } \\omega \\geq 0\\\\\n",
    "        0 & \\text{ else }\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "$ \\implies \\text{ the state space is given by } S = \\left( 0, 2 \\right)$. What is $E[M]$? What are $ \\mu_M [M=2]$ and $ \\mu_M [M=0]$?\n",
    "\n",
    "\n",
    "\n",
    "## **Question 2** (Monte-Carlo approximation)\n",
    "\n",
    "1. Write a Python program to estimate pi using Monte-Carlo approximation (Law of large number). Report a number of sample that allow you to obtain 2 digits of accuracy for pi. Hint: Drawing independently $x^i$, $x^i$ fron uniform distribution $\\mathcal{U}[0,1]$. Probability of points inside the circile of radius of 1 is $\\pi/4$.\n",
    "\n",
    "2. Notice that using elementary calculus the area of the first quarter of can also be computed as\n",
    "$$ \\frac{\\pi}{4} = \\int_0^1 \\sqrt(1-x^2) dx = E[\\sqrt(1-x^2)]$$\n",
    "where X is the uniform random variable U [0, 1]. Describe how you are going to use the Montecarlo (LLN) to estimate π/4 in this case. Write a Python code to estimate π. Report a number of sample that allow you to obtain 2 digits of accuracy for pi.\n",
    "\n",
    "3. Compare to the preceding methods, which approach converges faster (i.e. you get a smaller N)? Try to explain your answer. \n",
    "\n",
    "Hint: use need to run a few hunred of random seed cases and for each random seed drawing about 5000 or 10000 samples. Then, compute the expection (average over random seed cases the estimated $\\pi$) and variance of each method. from that you can explain correctly. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (linear regression + conditional distribution)\n",
    "\n",
    "Generating 20 some training data samples from a model with Gaussian distribution by\n",
    "$$y^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\epsilon^{(i)},$$\n",
    "where $\\theta_0 = 1, \\theta_1 = 2$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = 1$ is normal gaussian distribution.\n",
    "\n",
    "Given the generated data set $\\mathcal{D} (x^{(i)}, y^{(i)})_{i=1}^{20}$, we lean the the model $y^{(i)} = \\theta_0^* + \\theta_1^* x^{(i)}$.  -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (Binary classification + L2 ~ maximum likelihood, maximum a posterior) \n",
    "\n",
    "In assignment 2, we derived the gradient descent to train logisctic regression model that classifies two types of flowers from 4 different features. The loss function is considered with/without L2 regularization term. \n",
    "1. You need to derive and explain how to come up with the binary classification loss function (without L2 regularization) via the lens of condition maximum likelihood learning.\n",
    "2. You need to derive and explain how to come up with the binary classification loss function with L2 regularization via the lens of maximum a posterior learning. What is the prior knowledge of $\\theta$ is given in this scenario. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (Binary classification + L2 ~ maximum likelihood, maximum a posterior)\n",
    "\n",
    "1.) Proposing a polynormal function of any order from (2,6), $y = f(x) = ax^2 + bx + c$, (a,b,c) you should choose by yourself. First, you generate some N (you should pick a number N) noisy data samples by\n",
    "- Draw gaussian samples $x^i \\sim \\mathcal{N}(0, 2)$\n",
    "- Generate noisy y, i.e., $y^i = f(x^i) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\delta^2)$, you should pick small noise level $\\delta$\n",
    "\n",
    "2.a) From N samples $(x^i, y^i)$, finding the optimal $\\theta^*$ by optimizing the model using maximum likelihood function (you need to derive the maximum likelihood function). \n",
    "2.b) finding the mean of conditional distribution \n",
    "$$p(y | x; \\theta^*)$$\n",
    "as function of x and then plotting this relationship.\n",
    "\n",
    "2.c) finding the variance of conditional distribution $$p(y | x; \\theta^*)$$ as function of x, assumeing that $\\delta$ is the noise level above. And then, plot the bandwidth/stide ($y \\pm \\sigma$) on top of 2.b) figure.\n",
    "2.d) Verifying, drawing any $x^i \\sim \\mathcal{N}(0, 2)$, then computing $y^i = f_{\\theta^*}(x^i) + \\epsilon$. Plotting these pairs $x^i, y^i$ in the same figure, most of them should lie within the bandwidth.\n",
    "\n",
    "3.a) From N samples $(x^i, y^i)$, finding the optimal $\\theta^*$ by optimize the model using the MAP approach (you need to derive MAP function). The varience for prior Gaussian distribution of $\\theta$ should be a reasonable value (it has the same meaning as the regularization parameter)\n",
    "\n",
    "3.b.c.d) reapeating steps 2.b.c.d, then discuss the difference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question 4** (Bayesian Predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Consider $\\Omega = [-2, 2]$ and define \n",
    "$$P[B \\subset \\Omega] := \\int_{B} \\frac{1}{4} d \\omega, \\text{ and thus }\n",
    "P[d\\omega] := \\frac{1}{4} d \\omega,\n",
    "$$\n",
    "that is the elementary event $\\omega$ is the uniform random variable on $\\Omega$.\n",
    "Now, define $M(\\omega)$ as: \n",
    "\\begin{align}\n",
    "    &M(\\omega) =\n",
    "    \\begin{cases} \n",
    "        2 & \\text{ if } \\omega \\geq 0\\\\\n",
    "        0 & \\text{ else }\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "$ \\implies \\text{ the state space is given by } S = \\left( 0, 2 \\right).$\n",
    "By definition of expectation, we have\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{m} = E[M]  &= \\int_{S} m \\mu_M [M][d m] &&\\\\        \n",
    "                    &= \\int_{\\Omega} M(\\omega) P[d\\omega] \\\\\n",
    "                    &= \\int_{\\Omega} M(\\omega) \\frac{1}{4} d \\omega \\\\\n",
    "                    &= \\int_0^2 2 \\frac{1}{4} d \\omega\\\\\n",
    "                    &= \\frac{1}{2} 2 = 1\n",
    "\\end{align*} -->\n",
    "\n",
    "<!-- Recall: \n",
    "\\begin{enumerate}\n",
    "    \\item $M^{-1}\\LRp{d M} = d\\Omega$\n",
    "\\item $\\mu_M[M][B] = P [B]=P[M^{-1}\\LRp{B}]$\n",
    "    \\item $\\mu_M[M][S] = P[\\Omega] = 1$\n",
    "\\end{enumerate} \n",
    "Then\n",
    "\\begin{align*}\n",
    "\\int_{S} \\rv \\mu_M[M][d \\rv] = \\int_{\\Omega} M(\\omega) P[d\\omega] &= \\int_{\\Omega} M(\\omega) \\frac{1}{4} d \\omega \\\\\n",
    "                                                        &= \\int_0^2 2 \\frac{1}{4} d \\omega \\\\\n",
    "                                                        &= \\half 2 = 1\n",
    "\\end{align*} -->\n",
    "\n",
    "<!-- \\myorange{What are $ \\mu_M [M=2]$ and $ \\mu_M [M=0]$?} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. hidden the probability and challenge find back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
