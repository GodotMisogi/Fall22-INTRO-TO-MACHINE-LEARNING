{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Assigment 3</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Due day: 11:00 pm, Thursday, 06 October, 2022 </center></h1>\n",
    "#### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Probability + random variables)\n",
    "\n",
    "Comparing the Generative and Discriminative models\n",
    "\n",
    "## **Question 2** (Monte-Carlo approximation)\n",
    "\n",
    "Derive the optimal covariance in multiple classificaton problem.\n",
    "\n",
    "## \n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (linear regression + conditional distribution)\n",
    "\n",
    "Generating 20 some training data samples from a model with Gaussian distribution by\n",
    "$$y^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\epsilon^{(i)},$$\n",
    "where $\\theta_0 = 1, \\theta_1 = 2$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = 1$ is normal gaussian distribution.\n",
    "\n",
    "Given the generated data set $\\mathcal{D} (x^{(i)}, y^{(i)})_{i=1}^{20}$, we lean the the model $y^{(i)} = \\theta_0^* + \\theta_1^* x^{(i)}$.  -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (Binary classification + L2 ~ maximum likelihood, maximum a posterior) \n",
    "\n",
    "In assignment 2, we derived the gradient descent to train logisctic regression model that classifies two types of flowers from 4 different features. The loss function is considered with/without L2 regularization term. \n",
    "1. You need to derive and explain how to come up with the binary classification loss function (without L2 regularization) via the lens of condition maximum likelihood learning.\n",
    "2. You need to derive and explain how to come up with the binary classification loss function with L2 regularization via the lens of maximum a posterior learning. What is the prior knowledge of $\\theta$ is given in this scenario. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
