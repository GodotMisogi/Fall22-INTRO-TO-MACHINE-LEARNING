{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Assigment 4</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Due day: 11:00 pm, 2022 </center></h1>\n",
    "#### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Question 1**\n",
    "\n",
    "In lecture 7, we can maximize the likelihood \n",
    "$$\\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log P(x^{(i)} | y^{(i)} ; \\mu_k, \\Sigma_k)\n",
    "= \\max_{\\mu_k, \\Sigma_k} \\sum_{i : y^{(i)} = k} \\log \\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)$$\n",
    "over the Gaussian parameters. Prove that the empirical means and covariances of each class are\n",
    "\\begin{align*}\n",
    "\\mu_k & = \\frac{\\sum_{i: y^{(i)} = k} x^{(i)}}{n_k} \\\\\n",
    "\\Sigma_k & = \\frac{\\sum_{i: y^{(i)} = k} (x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^\\top}{n_k}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (linear regression + conditional distribution)\n",
    "\n",
    "Generating 20 some training data samples from a model with Gaussian distribution by\n",
    "$$y^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\epsilon^{(i)},$$\n",
    "where $\\theta_0 = 1, \\theta_1 = 2$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = 1$ is normal gaussian distribution.\n",
    "\n",
    "Given the generated data set $\\mathcal{D} (x^{(i)}, y^{(i)})_{i=1}^{20}$, we lean the the model $y^{(i)} = \\theta_0^* + \\theta_1^* x^{(i)}$.  -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ## **Question 3** (Binary classification + L2 ~ maximum likelihood, maximum a posterior) \n",
    "\n",
    "In assignment 2, we derived the gradient descent to train logisctic regression model that classifies two types of flowers from 4 different features. The loss function is considered with/without L2 regularization term. \n",
    "1. You need to derive and explain how to come up with the binary classification loss function (without L2 regularization) via the lens of condition maximum likelihood learning.\n",
    "2. You need to derive and explain how to come up with the binary classification loss function with L2 regularization via the lens of maximum a posterior learning. What is the prior knowledge of $\\theta$ is given in this scenario. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: (Generative VS Discriminative Models)\n",
    "\n",
    "We are going to use the Iris flower dataset. We will use three features sepal length (cm), sepal width (cm), petal length (cm) to classify types of flowers in generative model.\n",
    "1. Compute the emprical means and covariance for each type of flower.\n",
    "2. Generating new flowers, denoted as set $S$, from the learned proparbility and plotting these new flowers in 3D. Discuss your results.\n",
    "3. We assume that set $S$ is test data. Using logistic regression to training the model, then verify the learn model with test data $S$. Discuss you results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (beta-binomial model)\n",
    "Flipping a coin problem is a example of Bernouli distribution (a special case of Binomial distribution). And, beta distribution is a \"conjugate prior\" for Binomial distribution. In the lecutre 5, we have got the mean of the posterior distribution\n",
    "$$\\theta^*= \\frac{\\#\\,\\text{heads} + \\alpha}{\\#\\,\\text{heads}+\\#\\,\\text{tails} + \\alpha + \\beta}$$\n",
    "\n",
    "1. Finding the covariance of posterior distribution (Hint: convert to beta distribution). Discuss the relationship between the covariance and the number of samples.\n",
    "2. Finding the posterior preidctive distribution , i.e,\n",
    "$$p[x = \\text{H}|\\mathcal{D}] = \\int_0^1 p[x = \\text{H}|\\theta] \\  p[\\theta|\\mathcal{D}] d\\theta$$\n",
    "\n",
    "Recall: beta distribution\n",
    "\n",
    "$$\n",
    "\\text{Beta}(x,|a,b) = \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1},\n",
    "$$\n",
    "where $B(a,b)$ is a normalized constant. The mean and the variance are\n",
    "\n",
    "$$ \\text{mean} = \\frac{a}{a+b}, \\quad \\text{var} = \\frac{ab}{(a+b)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
