{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg6uOvMMTC-2"
   },
   "source": [
    "## <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "## <h1><center>Assigment 4</center></h1>\n",
    "### <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "#### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "#### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "##### <h1><center>Due day: 11:59 pm, 21 October, Friday, 2022 </center></h1>\n",
    "#### All solutions must be in one pdf file except codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRuEupb4TC-3"
   },
   "source": [
    "## **Question 1** (+30) (Probalistic linear regression )\n",
    "Generating 6 data samples for the problem by this polynomial functions (use the below given code for create $x^i$)\n",
    "$$ f(x) = .5 - {x} - .5 {x}^2 - 2 {x}^3 + 5 {x}^4$$\n",
    "Then adding noise:\n",
    "$y^i = f(x^i) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\delta^2)$, and noise level $\\delta = 0.2$\n",
    "\n",
    "The approximation model is 2nd order.\n",
    "\n",
    "In assignment 3, question 3. We have computed the posterior distribution by using $\\theta^*$ (the MAP point) to achieve the mean, and keep the variance the same as the noise level $\\delta^2$. \n",
    "\n",
    "1. Instead of that, using Bayesian Predictions, finding the mean and variance of \n",
    "$$ P(y | x, \\mathcal{D}) = \\int_\\theta P(y \\mid x, \\theta) P(\\theta \\mid \\mathcal{D}) d\\theta. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for Q1.1:\n",
    "\n",
    "I. All about component $\\prob{\\t|\\d}$.\n",
    "\n",
    "1. what is  $\\prob{\\t|\\d}$? The probability of parameter that is estimated by a given training data set $\\d$. \n",
    "\n",
    "2. What you have learned in the class about it? Case I: we can determin by mean square least error (you have learned the probalistis least square as well). Case II: where we use the MAP method to incoperate information of prior knowledge of $\\t$.\n",
    "\n",
    "3. Which one is require in this problem? You were asked to perform MAP.\n",
    "\n",
    "4. How can we do it? We use Bayesian formula,\n",
    "\n",
    "$$\\prob{\\t|\\d} = \\frac{\\prob{\\d|\\t} \\prob{\\t}}{\\prob{\\d}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. OMG! it looks complicated. hum, yes. Let break it down one by one\n",
    "\n",
    "    - $\\prob{\\t} = \\Dis{0}{\\tau^2}$ is the prior distribution of $\\t$ (Have you ever asked what does the prior distribution mean? Why we need it, how we know it? You might get some answer like you always have it, it is given, but why?)\n",
    "    - $\\prob{\\d|\\t}$ is the data distribution given a $\\t$ vector. \n",
    "\n",
    "        + what is our data? it is $\\d = \\LRp{y,x}$.\n",
    "        + what does distribution looks like? It is confused, hey! we have $y = \\t^T x + \\epsilon$, where $\\epsilon = \\Dis{0}{\\sigma^2}$\n",
    "        + Then, $\\prob{\\d|\\t} = \\prob{y|x, \\t} = \\Dis{\\t^T x}{\\sigma^2}$, how we get this one from the last sentences. (I do not know how explain it, you need to understand and remember it on your own way!)\n",
    "\n",
    "    - $\\prob{\\d}$ is data distribution. STOP! if we have it, we have eeverything, what else need to do? We just have a few training data points, how we can get it? **YOU DO NOT NEED IT** to get mean and variance of predictive model. Really?\n",
    "\n",
    "6. Let's accept that we do not have $\\prob{\\d}$, we denote the proportional probability as \n",
    "$$\\prob{\\t|\\d} \\propto {\\prob{\\d|\\t} \\prob{\\t}}$$\n",
    "Here \"proportional\" means you ignore a certain normalized constant (what is it?) of probability distribution. Note: we allow to ignore because the constant does not affect your final solution mean and variance. Really?\n",
    "\n",
    "7. We recall product of two gaussian distributions is a gaussian distribution (check it!), then we assume \n",
    "\n",
    "$$\\prob{\\t|\\d} = \\Dis{\\mu_\\t}{\\Sigma_\\t} \\propto \\exp\\LRp{||\\t - \\mu_\\t||_{\\Sigma_\\t^{-1}}^2}$$\n",
    "\n",
    "where $||\\t - \\mu_\\t||_{\\Sigma_\\t^{-1}}^2 = \\LRp{\\t - \\mu_\\t}^T \\Sigma_\\t^{-1}\\LRp{\\t - \\mu_\\t}$. No worries it is just a formula. Don't panic.\n",
    "\n",
    "8. Ok. From step 6. we can write something like\n",
    "\n",
    "$$ \\Dis{\\mu_\\t}{\\Sigma_\\t} \\propto \\Dis{\\t^T x}{\\sigma^2} \\Dis{0}{\\tau^2} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\exp\\LRp{\\frac{||\\t - \\mu_\\t||_{\\Sigma_\\t^{-1}}^2}{2}} \\propto \\exp\\LRp{\\frac{||y - x^T \\t||^2}{2\\sigma^2}} \\exp\\LRp{\\frac{||\\t||^2}{2\\tau^2}}$$\n",
    "\n",
    "9. How can I get $\\mu_\\t$ and $\\Sigma_\\t$ from the above relationship? \n",
    "    - Come back to the definition: the mean is the value in which the probability is highest => is it a special value? yes, it is the point where gradient of distribution function is zeros.  So let take gradient w.r.t. $\\t$ both sides and set it to zero.\n",
    "    - How about variance? We have the Gaussian is quadratic in $\\t$ and the quadratic term goes with the covariance matrix. for example,\n",
    "    $$||\\t - \\mu_\\t||_{\\Sigma_\\t^{-1}}^2 = \\LRp{\\t - \\mu_\\t}^T \\Sigma_\\t^{-1}\\LRp{\\t - \\mu_\\t}$$\n",
    "    How can we get it? let differentiate twice w.r.t. $\\t$\n",
    "\n",
    "10. Hey, we should differentiate the RHS (not the LHS) in step 8. Wait! it is exponential? Since they are equivalent, it is legal (check it!) to take log both side before differentiating \n",
    "\n",
    "$$\\ln\\LRp{RHS} = \\LRp{\\frac{||y - x^T \\t||^2}{\\sigma^2}} + \\LRp{\\frac{||\\t||^2}{\\tau^2}}$$\n",
    "\n",
    "$$ \\grad{\\t}{\\ln\\LRp{RHS}} = \\frac{-x(y - x^T \\t)}{\\sigma^2} + \\frac{\\t}{\\tau^2} = 0 \\implies \\t^* = \\LRp{\\frac{x x^T}{\\sigma^2} + \\frac{1}{\\tau^2}}^{-1}xy$$\n",
    "\n",
    "we note that $\\mu_\\t = \\t^*$\n",
    "\n",
    "Now, let twice differentiating RHS, we have\n",
    "\n",
    "$\\nabla_\\t \\grad{\\t}{\\ln\\LRp{RHS}} = \\frac{xx^T}{\\sigma^2} + \\frac{1}{\\tau^2} = \\Sigma_\\t^{-1} \\implies \\Sigma_\\t = \\LRp{\\frac{x x^T}{\\sigma^2} + \\frac{1}{\\tau^2}}^{-1}$\n",
    "\n",
    "11. Phew! Finally, we get\n",
    "\n",
    "$$\\prob{\\t|\\d} = \\Dis{\\LRp{\\frac{x x^T}{\\sigma^2} + \\frac{1}{\\tau^2}}^{-1}xy}{\\LRp{\\frac{x x^T}{\\sigma^2} + \\frac{1}{\\tau^2}}^{-1}}$$\n",
    "\n",
    "\n",
    "II. What is $\\prob{y|x,\\t}$, is it the same as step 5 in part I? NO! this is for predicting step. In words, what is the probability of test data set ($\\d_{\\text{test}} = y_{\\text{test}} | x_{\\text{test}}$, this is just my understanding so as to remember easier) given the training data set. I think it is better to write something like \n",
    "\n",
    "$$\\prob{y_{\\text{test}}|x_{\\text{test}},\\t}, $$\n",
    "\n",
    "and then we compute the predictive distribution\n",
    "\n",
    "$$ \\prob{y_{\\text{test}}|x_{\\text{test}}, \\d} = \\int  \\prob{y_{\\text{test}}|x_{\\text{test}},\\t} \\prob{\\t|\\d} d\\t$$\n",
    "\n",
    "Let assume\n",
    "\n",
    "$$\\prob{y_{\\text{test}}|x_{\\text{test}}, \\d} = \\Dis{\\mu_{y_{\\text{test}}|x_{\\text{test}}}}{\\Sigma_{y_{\\text{test}}|x_{\\text{test}}}} \\propto \\exp\\LRp{\\frac{||y_{\\text{test}} - \\mu_{y_{\\text{test}}|x_{\\text{test}}}||_{\\Sigma_{y_{\\text{test}}|x_{\\text{test}}}^{-1}}^2}{2}}$$\n",
    "\n",
    "III. How we determin that probability distribution? what is the integral? it is nothing more than maginalize distribution (what is this?) or we can think like expected value [see wiki reference](https://en.wikipedia.org/wiki/Marginal_distribution). Hence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\\mu_{y_{\\text{test}}|x_{\\text{test}}} = \\mathbb{E}_{\\t|\\d}\\LRs{\\prob{y_{\\text{test}}|x_{\\text{test}},\\t}} = x_{\\text{test}}^T \\mu_\\t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we can use the same trick as the step 10 part I, or using variable transformation $y_{\\text{test}} = x_{\\text{test}}^T \\t + \\epsilon $.\n",
    "Now for the variance\n",
    "$$\\Sigma_{y_{\\text{test}}|x_{\\text{test}}} = \\mathbb{\\text{Var}}\\LRs{\\prob{y_{\\text{test}}|x_{\\text{test}},\\t}} = x_{\\text{test}}^T \\Sigma_{\\t} x_{\\text{test}} + \\sigma^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\LRp}[1]{\\left( #1 \\right)}$ \n",
    "$\\newcommand{\\LRs}[1]{\\left[ #1 \\right]}$ \n",
    "$\\newcommand{\\LRa}[1]{\\left< #1 \\right>}$ \n",
    "$\\newcommand{\\LRc}[1]{\\left\\{ #1 \\right\\}}$ \n",
    "$\\newcommand{\\prob}[1]{P\\LRs{#1}}$\n",
    "$\\newcommand{\\Dis}[2]{\\mathcal{N}\\LRp{#1, #2}}$\n",
    "$\\newcommand{\\t}{\\theta}$\n",
    "$\\newcommand{\\d}{\\mathcal{D}}$\n",
    "$\\newcommand{\\grad}[2]{\\nabla_{#1}\\LRp{#2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
