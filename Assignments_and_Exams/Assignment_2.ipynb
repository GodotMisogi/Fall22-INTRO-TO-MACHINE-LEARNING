{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "# <h1><center>Assigment 2</center></h1>\n",
    "## <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "#### <h1><center>Due day: 11:00 pm, Thursday, 20 September, 2022 </center></h1>\n",
    "### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Linear regression + regularization techniques + data augmentation) (+50 bonus)\n",
    "\n",
    "In this problem, we approximate the polynomial from noisy data points. To that end, we generate a data pair $(x^{(i)}, y^{(i)})$ from the following noise-corrupted relation  \n",
    "\n",
    "$$ y^{(i)} = .5 - {x^{(i)}} - .5 {x^{(i)}}^2 - 2 {x^{(i)}}^3 + 5 {x^{(i)}}^4 + \\delta \\varphi^{(i)} x^{(i)}$$\n",
    "of the following  ground truth function\n",
    "$$ y = .5 - {x} - .5 {x}^2 - 2 {x}^3 + 5 {x}^4$$\n",
    "\n",
    "where $\\delta = 0.05$ is the noise level, $\\varphi$ is drawn from standard normal distribution. We need to generate 8 holdout samples and 7 training samples.\n",
    "\n",
    "- (1.a) (+5) Fill the generating data code and produce figures to visualize data and true functions.\n",
    "- (1.b) (+5) Perform the LinearRegression() using polynomial approximation of order of 1, 10 and 4. Visualizing each cases to see which case are overfitting, underfitting and good fit. Adding the mean square error of the holdout data to support your observation.\n",
    "- (1.c) (+10) For overfitting senario, applying L2 regularization. You need to do: \n",
    "    + step 1: Plot the L2 regularization model versus no regularization model versus the ground truth function in one plot to see how L2 works (pick a reasonable $\\lambda$).\n",
    "    + step 2: Plot the mean square error of holdout data as a function of the regularization parameter $\\lambda$ over the range $\\lambda \\in [10^{-5},10]$. What is the optimal $\\lambda$ approximately? \n",
    "    + step 3: Using L-curved technique (read [https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf]) to find out the optimal $\\lambda$. The L-curved line is the one represents the relationship between residual norm $||y - f(x,\\theta)||_2$ and solution norm $||\\theta||_2$. Plot the L-curve and compare the optimal values of $\\lambda$ from step 2 and step 3?\n",
    "- (1.d) (+10) For overfitting senario, applying L1 regularization. Repeat the all 3 steps in the question (1.c).\n",
    "- (1.e) (+10) For overfitting senario, applying Elasticnet regularization (a mixture of L1 and L2), i.e., the loss function with Elasticnet regularization is \n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\lambda \\cdot ||\\theta||_1 + \\frac{1}{2} \\gamma ||\\theta||_2^2$$\n",
    "\n",
    "We are interested in the performance of the mean square error of the holdout data as a function of $\\lambda$ and $\\gamma$. One way to achieve this is to plot the contour of the mean square error of the holdout data as a function of $\\lambda$ and $\\gamma$. Discuss the relationship between Elasticnet and L2, L1 regularization?\n",
    "\n",
    "- (1.g) (bonous +10) The question is **can we do something else** and yet achieve the same regularization effect? One way to accomplish this is via **data augmentation technique**. The data augmentation technique that we study consists of 4 steps:\n",
    "    + step 1: Clone 7 training data samples (X in the code) into 350 samples (using numpy.repeat). (Note: this is noise-free data).\n",
    "    + step 2: We add noise these 350 samples by $$\\tilde{x}^i = x^i + \\delta \\varphi^i,$$ where $\\varphi^i$ is drawn from standard normal distribution. We get X_data_augmented.\n",
    "    + step 3: Similarly, clone 7 training label data into 350 samples (using numpy.repeat), denoted y_data_augmented. \n",
    "    + step 4: Using Linearregression to train using data augmented data pairs (X_data_augmented, y_data_augmented).\n",
    "\n",
    "    You need to (1) generate augmented data, (2) Determine a good noise level $\\delta$ by trying different values of $\\delta$? Explain why too much or too small noise is not good? To answer the last question, you may need to read the folllowing references\n",
    "    1. Train Neural Networks With Noise to Reduce Overfitting, [https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/]\n",
    "    2. mcTangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso, LogisticRegression\n",
    "\n",
    "## please keep the below codes for generating x, DO NOT CHANGE!\n",
    "# ==========================================================================================\n",
    "# holdout samples\n",
    "n_holdout_samples = 8\n",
    "X_holdout = np.sort(np.random.rand(n_holdout_samples)) \n",
    "\n",
    "# Training samples\n",
    "n_samples = 7\n",
    "X = np.sort(np.random.rand(n_samples)) \n",
    "# ==========================================================================================\n",
    "\n",
    "# [CONTINUE YOUR WORK FROM HERE!]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2** (Logistic regression) (+30)\n",
    "\n",
    "In this problem, we will use regression to  classify three types of iris flowers ('Iris Setosa', 'Iris Versicolour', 'Iris Virginica') based on 4 features (petal length, petal width, sepal length, sepal width). Recall that for linear regression our model class is given as\n",
    "$$\n",
    "y = \\theta^T x.\n",
    "$$\n",
    "where $\\theta = \\{\\theta_0, ..., \\theta_n\\}$, and $x = \\{1, x_1, ..., x_n\\}$ with $n$ as the number of input attributes/features.\n",
    "For the logistic regression, the model class reads\n",
    "$$\n",
    "y = \\frac{1}{1 + e^{\\theta^T x}}.\n",
    "$$\n",
    "For logistic regression, we need to define a new loss function (to accommodate the nature of the classification problem). We choose to use the cross entropy loss which is given as\n",
    "\n",
    "1. Derive the gradient expression of the loss function for logistic regression\n",
    "2. Performing Logistic regression using the gradient descent codes that you wrote in the first homework. Plot your classfication on the testing data and report the percentage of misclassifications.\n",
    "3. Performing Logistic regression with L2 regularization using the gradient descent codes that you wrote in the first homework. Plot your classfication on the testing data and report the percentage of misclassifications.\n",
    "4. Performing Logistic regression, using scikit-learn function ...., compare the results with question 2.\n",
    "5. erforming Logistic regression with L2 regularization , using scikit-learn function ...., compare the results with question 2.\n",
    "6. Comparing the number of wrongly predicted points from question 2 and question. Discuss the reason for your answer.\n",
    "\n",
    "You need to test the model on the same data and then visualizing the flower data points that are misclassified (circling the point with figure in 2.a, see lecture 1 for diabetes risk example).\n",
    "\n",
    "- (2.a) (+5) Visualize 'Iris Setosa', 'Iris Versicolour', 'Iris Virginica' flowers with respect to petal length (x-axis) and petal width (y-axis), see lecture 1.\n",
    "- (2.b) (+10) Performing Logistic regression, using ....,  without regularization to learn the model. You need to test the model on the same data and then visualizing the flower data points that are misclassified (circling the point with figure in 2.a, see lecture 1 for diabetes risk example).\n",
    "- (2.c) (+5) Performing Logistic regression with L2 regularization. You need to test the model on the same data and then visualize the flower data points that are misclassified.\n",
    "- (2.d) (+5) Performing Logistic regression with L1 regularization (using *liblinear* solver, see ...). You need to test the model on the same data and then visualize the flower data points that are misclassified.\n",
    "- (2.e) (+5) Comparing the number of wrongly predicted points from (2c) and (2.d) with the one from (2.b). What is the reason for that result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load and visualize the Iris flower dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Visualize data\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.ylabel(\"petal width (cm)\")\n",
    "# plt.xlabel(\"petal length (cm)\")\n",
    "# plt.title(\"Dataset of Iris flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (Probability) (+20)\n",
    "\n",
    "(+20) Prove that if $$A \\subseteq B$$ then $$\\mathcal{P} [A] \\le \\mathcal{P} [B].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
