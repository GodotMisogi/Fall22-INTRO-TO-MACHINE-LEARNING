{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Course: Introduction to Machine Learning</center></h1>\n",
    "# <h1><center>Assigment 2</center></h1>\n",
    "## <h1><center>Important NOTE: In order to get full grades, for every question, you need to provide the details of your work on how to get to a solution or the end of the proof</center></h1>\n",
    "### <h1><center>Instructor: Tan Bui-Thanh</center></h1>\n",
    "### <h1><center>TA: Hai Nguyen</center></h1>\n",
    "#### <h1><center>Due day: 11:00 pm, Thursday, 20 September, 2022 </center></h1>\n",
    "### If you gonna use this jupyter notebook for the assignment, please convert to .pdf file for submission. All submission must be in pdf format except codes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1** (Linear regression + regularization techniques + data augmentation) (+50 +20 bonous)\n",
    "\n",
    "In this problem, we approximate the polynomial from noisy data points. To that end, we generate a data pair $(x^{(i)}, y^{(i)})$ by  \n",
    "\n",
    "$$ y^{(i)} = .5 - {x^{(i)}} - .5 {x^{(i)}}^2 - 2 {x^{(i)}}^3 + 5 {x^{(i)}}^4 + \\delta \\varphi^{(i)} x^{(i)},$$\n",
    "\n",
    "where $\\delta = 0.05$ is the noise level, $\\varphi$ is drawn from standard normal distribution. We need to generate 8 holdout samples and 7 training samples.\n",
    "\n",
    "- (1.a) (+5) Filling the generating data code and visualize data and true functions.\n",
    "- (1.b) (+5) Performing the LinearRegression() using polynomial approximation of order of 1, 10 and 4. Visualizing each cases to see which case are overfitting, underfitting and good fit. Adding the mean square error of the holdout data for convincing.\n",
    "- (1.c) (+10) For overfitting senario, applying L2 regularization. You need to do: \n",
    "    + step 1: Plot the L2 regularization model versus no regularization model and true function in one plot to see how L2 works (pick a reasonable $\\lambda$).\n",
    "    + step 2: Plot the relationship between mean square error of holdout data and regularization parameter $\\lambda$. What is the optimal $\\lambda$? \n",
    "    + step 3: Using L-curved technique to find out the optimal $\\lambda$. The L-curved line is the one represents the relationship between residual norm $||y - f(x,\\theta)||_2$ and solution norm $||||_2$. Where does the optimal parameter obtained in step 2 lie in the L-curved line?\n",
    "- (1.d) (+10) For overfitting senario, applying L1 regularization. Repeat the all 3 steps in the question (1.c).\n",
    "- (1.e) (+10) You will find it interesting if you search the L-curved techniques to determined the regularization parameter for inverse problems. Does your optimal points in step 3 (in 1.c, 1.d) match with the theory of original L-curved theory? (L-curved method [https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf])\n",
    "- (1.f) (+10) For overfitting senario, applying Elasticnet regularization (a mixture of L1 and L2), i.e., the loss function with Elasticnet regularization is \n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(y^{(i)}, \\theta^\\top x^{(i)}) + \\lambda \\cdot ||\\theta||_1 + \\frac{1}{2} \\gamma ||\\theta||_2^2$$\n",
    "\n",
    "You need to plot the contour with x-axis is $\\lambda$, y-axis is $\\gamma$ and value of function is the mean square error of holdout data. Can you make commments on the relationship between Elasticnet and L2, L1 regularization?\n",
    "\n",
    "- (1.g) (bonous +10) The question is **can we do something else** and then get the same regularization effect? The answer is **data augmentation technique**. The data augmentation techniques are\n",
    "    + step 1: Clone 7 (X) samples data to 350 samples (using numpy.repeat). (Note: this is clean data - noise free).\n",
    "    + step 2: We add noise the 350 $x^i$ sample by $$\\tilde{x}^i = x^i + \\delta \\varphi^i,$$ where $\\varphi^i$ is drawn from standard normal distribution. We get X_data_augmented.\n",
    "    + step 3: Clone 7 (f(X)) samples data to 350 samples (using numpy.repeat), denoted y_data_augmented. \n",
    "    + step 4: Using Linearregression to get model from data augmented data pairs (X_data_augmented, y_data_augmented).\n",
    "\n",
    "    You need to (1) generate augmented data, (2) What is a good noise level $\\delta$? Can you explain why too much/small noise is not good? (Train Neural Networks With Noise to Reduce Overfitting, [https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/])\n",
    "\n",
    "Note: you might get +10 bonous points more if you answer is good enough for L-curved method in (1c-e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso, LogisticRegression\n",
    "\n",
    "## please keep the below codes for generating x, DO NOT CHANGE!\n",
    "# ==========================================================================================\n",
    "# holdout samples\n",
    "n_holdout_samples = 8\n",
    "X_holdout = np.sort(np.random.rand(n_holdout_samples)) \n",
    "\n",
    "# Training samples\n",
    "n_samples = 7\n",
    "X = np.sort(np.random.rand(n_samples)) \n",
    "# ==========================================================================================\n",
    "\n",
    "# [CONTINUE YOUR WORK FROM HERE!]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2** (Logistic regression) (+30)\n",
    "\n",
    "In this problem, we will learn the model that classifies three types of iris flowers ('Iris Setosa', 'Iris Versicolour', 'Iris Virginica') based on 4 features (petal length, petal width, sepal length, sepal width).\n",
    "- (2.a) (+5) Visualize 'Iris Setosa', 'Iris Versicolour', 'Iris Virginica' flowers with respect to petal length (x-axis) and petal width (y-axis), see lecture 1.\n",
    "- (2.b) (+10) Performing Logistic regression without regularization to learn the model. You need to test the model on the same data and then visualizing the flower data points that are misclassified (circling the point with figure in 2.a, see lecture 1 for diabetes risk example).\n",
    "- (2.c) (+5) Performing Logistic regression with L2 regularization. You need to test the model on the same data and then visualizing the flower data points that are misclassified.\n",
    "- (2.d) (+5) Performing Logistic regression with L1 regularization (using *liblinear* solver, look at scikit-learn). You need to test the model on the same data and then visualizing the flower data points that are misclassified.\n",
    "- (2.e) (+5) Comparing the number of wrongly predicted points from (2c) and (2.d) with the one from (2.b). What is the reason for that result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load and visualize the Iris flower dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Visualize data\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.ylabel(\"petal width (cm)\")\n",
    "# plt.xlabel(\"petal length (cm)\")\n",
    "# plt.title(\"Dataset of Iris flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3** (Probability) (+20)\n",
    "\n",
    "(+20) Prove that if $$A \\subseteq B$$ then $$\\mathcal{P} [A] \\le \\mathcal{P} [B].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
